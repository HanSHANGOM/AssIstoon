{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3Hgvq5eccLfM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow-gpu\n",
      "  Downloading https://files.pythonhosted.org/packages/81/d1/9222b9aac2fa27dccaef38917cde84c24888f3cd0dd139c7e12be9f49a7a/tensorflow_gpu-1.14.0-cp37-cp37m-win_amd64.whl (287.7MB)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in c:\\users\\sanghoon\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (1.11.2)\n",
      "Requirement already satisfied: six>=1.10.0 in c:\\users\\sanghoon\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (1.12.0)\n",
      "Collecting google-pasta>=0.1.6 (from tensorflow-gpu)\n",
      "  Using cached https://files.pythonhosted.org/packages/d0/33/376510eb8d6246f3c30545f416b2263eee461e40940c2a4413c711bdf62d/google_pasta-0.1.7-py3-none-any.whl\n",
      "Collecting keras-applications>=1.0.6 (from tensorflow-gpu)\n",
      "  Using cached https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl\n",
      "Collecting keras-preprocessing>=1.0.5 (from tensorflow-gpu)\n",
      "  Using cached https://files.pythonhosted.org/packages/28/6a/8c1f62c37212d9fc441a7e26736df51ce6f0e38455816445471f10da4f0a/Keras_Preprocessing-1.1.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: absl-py>=0.7.0 in c:\\users\\sanghoon\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (0.7.1)\n",
      "Collecting gast>=0.2.0 (from tensorflow-gpu)\n",
      "  Using cached https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
      "Requirement already satisfied: protobuf>=3.6.1 in c:\\users\\sanghoon\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (3.9.0)\n",
      "Requirement already satisfied: tensorboard<1.15.0,>=1.14.0 in c:\\users\\sanghoon\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (1.14.0)\n",
      "Collecting tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 (from tensorflow-gpu)\n",
      "  Using cached https://files.pythonhosted.org/packages/3c/d5/21860a5b11caf0678fbc8319341b0ae21a07156911132e0e71bffed0510d/tensorflow_estimator-1.14.0-py2.py3-none-any.whl\n",
      "Collecting termcolor>=1.1.0 (from tensorflow-gpu)\n",
      "  Using cached https://files.pythonhosted.org/packages/8a/48/a76be51647d0eb9f10e2a4511bf3ffb8cc1e6b14e9e4fab46173aa79f981/termcolor-1.1.0.tar.gz\n",
      "Requirement already satisfied: grpcio>=1.8.6 in c:\\users\\sanghoon\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (1.22.0)\n",
      "Requirement already satisfied: wheel>=0.26 in c:\\users\\sanghoon\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (0.33.4)\n",
      "Collecting astor>=0.6.0 (from tensorflow-gpu)\n",
      "  Using cached https://files.pythonhosted.org/packages/d1/4f/950dfae467b384fc96bc6469de25d832534f6b4441033c39f914efd13418/astor-0.8.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: numpy<2.0,>=1.14.5 in c:\\users\\sanghoon\\anaconda3\\lib\\site-packages (from tensorflow-gpu) (1.16.4)\n",
      "Requirement already satisfied: h5py in c:\\users\\sanghoon\\anaconda3\\lib\\site-packages (from keras-applications>=1.0.6->tensorflow-gpu) (2.9.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\sanghoon\\anaconda3\\lib\\site-packages (from protobuf>=3.6.1->tensorflow-gpu) (41.0.1)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in c:\\users\\sanghoon\\anaconda3\\lib\\site-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu) (0.15.4)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\sanghoon\\anaconda3\\lib\\site-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu) (3.1.1)\n",
      "Building wheels for collected packages: gast, termcolor\n",
      "  Building wheel for gast (setup.py): started\n",
      "  Building wheel for gast (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\sanghoon\\AppData\\Local\\pip\\Cache\\wheels\\5c\\2e\\7e\\a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
      "  Building wheel for termcolor (setup.py): started\n",
      "  Building wheel for termcolor (setup.py): finished with status 'done'\n",
      "  Stored in directory: C:\\Users\\sanghoon\\AppData\\Local\\pip\\Cache\\wheels\\7c\\06\\54\\bc84598ba1daf8f970247f550b175aaaee85f68b4b0c5ab2c6\n",
      "Successfully built gast termcolor\n",
      "Installing collected packages: google-pasta, keras-applications, keras-preprocessing, gast, tensorflow-estimator, termcolor, astor, tensorflow-gpu\n",
      "Successfully installed astor-0.8.0 gast-0.2.2 google-pasta-0.1.7 keras-applications-1.0.8 keras-preprocessing-1.1.0 tensorflow-estimator-1.14.0 tensorflow-gpu-1.14.0 termcolor-1.1.0\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    import matplotlib\n",
    "    # Agg backend runs without a display\n",
    "    matplotlib.use('Agg')\n",
    "    import matplotlib.pyplot as plt\n",
    "  \n",
    "!pip install tensorflow-gpu "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tBBv0S5sc3mK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.14.0\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AS3z6BrcjR2P"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mNLE4JaTe4BP"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HpuYHzlddxqh"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sanghoon\\AssIstoon\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Mask R-CNN\n",
    "Train on the nuclei segmentation dataset from the\n",
    "Kaggle 2018 Data Science Bowl\n",
    "https://www.kaggle.com/c/data-science-bowl-2018/\n",
    "\n",
    "Licensed under the MIT License (see LICENSE for details)\n",
    "Written by Waleed Abdulla\n",
    "\n",
    "------------------------------------------------------------\n",
    "\n",
    "Usage: import the module (see Jupyter notebooks for examples), or run from\n",
    "       the command line as such:\n",
    "\n",
    "    # Train a new model starting from ImageNet weights\n",
    "    python3 Manga.py train --dataset=/path/to/dataset --subset=train --weights=imagenet\n",
    "\n",
    "    # Train a new model starting from specific weights file\n",
    "    python3 Manga.py train --dataset=/path/to/dataset --subset=train --weights=/path/to/weights.h5\n",
    "\n",
    "    # Resume training a model that you had trained earlier\n",
    "    python3 Manga.py train --dataset=/path/to/dataset --subset=train --weights=last\n",
    "\n",
    "    # Generate submission file\n",
    "    python3 Manga.py detect --dataset=/path/to/dataset --subset=train --weights=<last or /path/to/weights.h5>\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import datetime\n",
    "import numpy as np\n",
    "import skimage.io\n",
    "from imgaug import augmenters as iaa\n",
    "\n",
    "\n",
    "# Root directory of the project\n",
    "ROOT_DIR = os.path.abspath(\"../\")\n",
    "print(ROOT_DIR)\n",
    "# Import Mask RCNN\n",
    "sys.path.append(ROOT_DIR)  # To find local version of the library\n",
    "from mrcnn.config import Config\n",
    "from mrcnn import utils\n",
    "from mrcnn import model as modellib\n",
    "from mrcnn import visualize\n",
    "\n",
    "# Path to trained weights file\n",
    "#COCO_WEIGHTS_PATH = os.path.join('/content/dataset/logs/manga20190730T1026', \"mask_rcnn_manga_0433.h5\")\n",
    "\n",
    "COCO_WEIGHTS_PATH = os.path.join(ROOT_DIR, \"mask_rcnn_manga_2000.h5\") #모델의 경로\n",
    "\n",
    "DEFAULT_LOGS_DIR = os.path.join(ROOT_DIR, \"logs\") #필요없음\n",
    "\n",
    "RESULTS_DIR = os.path.join(ROOT_DIR, \"results/Manga/\") #저장할 이미지의 경로\n",
    "\n",
    "\n",
    "VAL_IMAGE_IDS = []# 여기에 이미지 디렉토리 이미지 이름 확장자 제외하고 리스트화\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hTylOkAViDXK"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# In[10]:\n",
    "\n",
    "\n",
    "############################################################\n",
    "#  Configurations\n",
    "############################################################\n",
    "\n",
    "class MangaConfig(Config):\n",
    "\n",
    "    \"\"\"Configuration for training on the Manga segmentation dataset.\"\"\"\n",
    "    # Give the configuration a recognizable name\n",
    "    NAME = \"Manga\"\n",
    "    IMAGE_RESIZE_MODE='pad64'\n",
    "    IMAGE_MIN_DIM = 768\n",
    "    IMAGE_MAX_DIM = 1216\n",
    "    GPU_COUNT = 1\n",
    "    # Adjust depending on your GPU memory\n",
    "    IMAGES_PER_GPU = 2\n",
    "    #LOSS_WEIGHTS = {\n",
    "    #\"rpn_class_loss\": 0.0113,\n",
    "    #\"rpn_bbox_loss\": 0.0464,\n",
    "    #\"mrcnn_class_loss\": 0.0206,\n",
    "    #\"mrcnn_bbox_loss\": 0.0157,\n",
    "    #\"mrcnn_mask_loss\": 0.1544}\n",
    "    '''\n",
    "    \n",
    "    rpn_class_loss: 0.0113 - rpn_bbox_loss: 0.0464 - mrcnn_class_loss: 0.0206 - mrcnn_bbox_loss: 0.0157 - mrcnn_mask_loss: 0.1544\n",
    "    '''\n",
    "    LEARNING_RATE=0.0001\n",
    "    TRAIN_BN = False\n",
    "    BACKBONE=\"resnet50\"\n",
    "    # Number of classes (including background)\n",
    "    NUM_CLASSES = 1 + 3  # Background + Manga\n",
    "    \n",
    "    USE_MINI_MASK = True\n",
    "    MINI_MASK_SHAPE = (112, 112)\n",
    "\n",
    "    # Number of training and validation steps per epoch\n",
    "    STEPS_PER_EPOCH = 8\n",
    "    VALIDATION_STEPS = 5\n",
    "\n",
    "    \n",
    "    MAX_GT_INSTANCES=30\n",
    "    # Don't exclude based on confidence. Since we have two classes\n",
    "    # then 0.5 is the minimum anyway as it picks between Manga and BG\n",
    "    DETECTION_MIN_CONFIDENCE = 0.9\n",
    "    RPN_NMS_THRESHOLD = 0.9\n",
    "    #BATCH_SIZE = 1\n",
    "\n",
    "class MangaInferenceConfig(MangaConfig):\n",
    "    IMAGE_RESIZE_MODE = \"pad64\"\n",
    "    def __init__(self,dataset=None):\n",
    "        \n",
    "        self.h=2112\n",
    "        self.w=1344\n",
    "\n",
    "        self.IMAGE_CHANNEL_COUNT=3\n",
    "        \"\"\"Set values of computed attributes.\"\"\"\n",
    "        # Effective batch size\n",
    "        self.BATCH_SIZE = 1\n",
    "        self.IMAGE_SHAPE = np.array([self.h, self.w,\n",
    "                self.IMAGE_CHANNEL_COUNT])\n",
    "        self.IMAGE_META_SIZE = 1 + 3 + 3 + 4 + 1 + self.NUM_CLASSES\n",
    "    # Set batch size to 1 to run one image at a time\n",
    "\n",
    "    \n",
    "    GPU_COUNT = 1\n",
    "    IMAGES_PER_GPU = 1\n",
    "    #len(VAL_IMAGE_IDS)\n",
    "    #BATCH_SIZE = len(VAL_IMAGE_IDS)\n",
    "    # Don't resize imager for inferencing\n",
    "    \n",
    "    DETECTION_MIN_CONFIDENCE = 0.9\n",
    "    # Non-max suppression threshold to filter RPN proposals.\n",
    "    # You can increase this during training to generate more propsals.\n",
    "    RPN_NMS_THRESHOLD = 0.9\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "akuevhlKuw7W"
   },
   "outputs": [],
   "source": [
    "# In[18]:\n",
    "\n",
    "\n",
    "\n",
    "############################################################\n",
    "#  Dataset\n",
    "############################################################\n",
    "\n",
    "class MangaDataset(utils.Dataset):\n",
    "    def __init__(self):\n",
    "        super(MangaDataset,self).__init__()\n",
    "    \n",
    "\n",
    "    def load_Manga(self,\n",
    "                   dataset_dir,\n",
    "                   subset,config=None): \n",
    "        \"\"\"망가데이터셋의 서브셋 불러오기.\n",
    "\n",
    "        dataset_dir: 데이터셋의 루트 디렉토리\n",
    "        subset: 불러올 서브셋. 서브디렉토리의 이름,\n",
    "                예를 들면 stage1_train, stage1_test, ...etc. 또는, 다음 중 하나:\n",
    "                * train: stage1_train excluding validation images\n",
    "                * val: VAL_IMAGE_IDS에 해당하는 validation 이미지\n",
    "        \"\"\"\n",
    "        # 클래스를 추가한다\n",
    "        # Naming the dataset Manga, and the class Manga\n",
    "        self.add_class(\"Manga\", 1, \"sfx\")\n",
    "        self.add_class(\"Manga\", 2, \"text\")\n",
    "        self.add_class(\"Manga\", 3, \"face\")\n",
    "\n",
    "        # 어떤 subset? val or train?\n",
    "        # \"val\": use hard-coded list above\n",
    "        # \"train\": use data from stage1_train minus the hard-coded list above\n",
    "        # else: use the data from the specified sub-directory\n",
    "        #assert subset in [\"train\", \"val\", \"stage1_train\", \"stage1_test\", \"stage2_test\"]\n",
    "        subset_dir = \"train\" if subset in [\"train\", \"val\"] else subset #수정필요\n",
    "        #dataset_dir = 'C:/Users/sanghoon/AssIstoon/sfx_extract/dataset/'\n",
    "        root_dir= dataset_dir+subset+'/'\n",
    "        images_dir = root_dir+'images/'\n",
    "        \n",
    "        if subset==\"train\" :\n",
    "            image_ids = [name[:-4] for name in os.listdir(images_dir)]  # 디렉토리 이름에서 ? 이미지 아이디를 받는다.\n",
    "        elif subset==\"val\":\n",
    "            image_ids=VAL_IMAGE_IDS\n",
    "        else :\n",
    "            image_ids = [name[:-4] for name in os.listdir(images_dir)]\n",
    "        # Add images\n",
    "        for idx,image_id in enumerate(image_ids):\n",
    "            image_array=skimage.io.imread(images_dir+image_id+'.jpg')\n",
    "            self.add_image(\"Manga\",image_id,images_dir+'{}.jpg'.format(image_id),index=idx,shape=image_array.shape,image=image_array)\n",
    "                ##padding 한 이미지\n",
    "                \n",
    "    def load_image(self, image_id):\n",
    "        \"\"\"Load the specified image and return a [H,W,3] Numpy array.\n",
    "        \"\"\"\n",
    "        # Load image\n",
    "        image = self.image_info[image_id]['image']\n",
    "        # If grayscale. Convert to RGB for consistency.\n",
    "        if image.ndim != 3:\n",
    "            image = skimage.color.gray2rgb(image)\n",
    "        # If has an alpha channel, remove it for consistency\n",
    "        if image.shape[-1] == 4:\n",
    "            image = image[..., :3]\n",
    "            image_dtype = image.dtype\n",
    "        # Default window (y1, x1, y2, x2) and default scale == 1.\n",
    "        h, w = image.shape[:2]\n",
    "        window = (0, 0, h, w)\n",
    "        scale = 1\n",
    "        padding = [(0, 0), (0, 0), (0, 0)]\n",
    "        if h % 64 > 0:\n",
    "            max_h = h - (h % 64) + 64\n",
    "            top_pad = (max_h - h) // 2\n",
    "            bottom_pad = max_h - h - top_pad\n",
    "        else:\n",
    "            top_pad = bottom_pad = 0\n",
    "        # Width\n",
    "        if w % 64 > 0:\n",
    "            max_w = w - (w % 64) + 64\n",
    "            left_pad = (max_w - w) // 2\n",
    "            right_pad = max_w - w - left_pad\n",
    "        else:\n",
    "            left_pad = right_pad = 0\n",
    "        padding = [(top_pad, bottom_pad), (left_pad, right_pad), (0, 0)]\n",
    "        image = np.pad(image, padding, mode='constant', constant_values=0)\n",
    "        window = (top_pad, left_pad, h + top_pad, w + left_pad)\n",
    "\n",
    "        return image\n",
    "    \n",
    "    def load_mask(self, image_id):\n",
    "        \n",
    "        info = self.image_info[image_id] #image id에 대한 info를 얻어온다.\n",
    "\n",
    "        # 이미지 경로로부터 마스크 디렉토리를 얻어온다.\n",
    "        mask_dir = os.path.join(info['root'], \"masks\")\n",
    "        # png이미지로부터 mask파일을 읽는다.\n",
    "        mask_list = [f for f in os.listdir(mask_dir) if\n",
    "                     f.split('_')[0] == info['id'].split('_')[0] + info['id'].split('_')[1] and f.endswith(\".png\")]\n",
    "\n",
    "        mask=[]\n",
    "        maskclasslist=[]\n",
    "        for f in mask_list:\n",
    "            maskclass=f.split('_')[1].split('-')[0]\n",
    "            if maskclass == 'sfx':\n",
    "                maskclasslist.append(1)\n",
    "            elif maskclass == 'text':\n",
    "                maskclasslist.append(2)\n",
    "            elif maskclass == \"face\":\n",
    "                maskclasslist.append(3)\n",
    "            m = skimage.io.imread(os.path.join(mask_dir, f))\n",
    "            m = m.sum(axis=2)\n",
    "\n",
    "            #mask[m==True]=1\n",
    "            mask.append(m)\n",
    "        #mask=mask.astype(np.bool)\n",
    "        #print(mask.shape)\n",
    "        mask = np.stack(mask, axis=-1)\n",
    "\n",
    "\n",
    "        # Return mask, and array of class IDs of each instance. Since we have\n",
    "        # one class ID, we return an array of ones\n",
    "        return mask, np.array(maskclasslist, dtype=np.int32)\n",
    "\n",
    "    def image_reference(self, image_id):\n",
    "        \"\"\"이미지의 경로를 반환한다.\"\"\"\n",
    "        info = self.image_info[image_id]\n",
    "        if info[\"source\"] == \"Manga\":\n",
    "            return info[\"id\"]\n",
    "        else:\n",
    "            super(self.__class__, self).image_reference(image_id)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wV51KNmgu037"
   },
   "outputs": [],
   "source": [
    "# In[19]:\n",
    "\n",
    "\n",
    "\n",
    "############################################################\n",
    "#  Training\n",
    "############################################################\n",
    "\n",
    "def train(model, dataset_dir=None):\n",
    "    \"\"\"Train the model.\"\"\"\n",
    "    # Training dataset.\n",
    "    dataset_train = MangaDataset()\n",
    "    dataset_train.load_Manga(dataset_dir,\"train\")\n",
    "    dataset_train.prepare() #dataset 클래스의 참조 필요\n",
    "\n",
    "    # Validation dataset\n",
    "    dataset_val = MangaDataset()\n",
    "    dataset_val.load_Manga(dataset_dir, \"val\")\n",
    "    dataset_val.prepare()\n",
    "\n",
    "    # Image augmentation\n",
    "    # http://imgaug.readthedocs.io/en/latest/source/augmenters.html\n",
    "    augmentation = iaa.SomeOf((0, 2), [\n",
    "        iaa.Fliplr(0.5),\n",
    "        iaa.Flipud(0.5),\n",
    "        #iaa.OneOf([iaa.Affine(rotate=90),\n",
    "        #           iaa.Affine(rotate=180),\n",
    "        #           iaa.Affine(rotate=270)]),\n",
    "        iaa.Multiply((0.8, 1.5)),\n",
    "        iaa.GaussianBlur(sigma=(0.0, 5.0))\n",
    "    ])#이건 뭐하는걸까? image augmentation인걸 보니 전처리를 해주는가보다.\n",
    "\n",
    "    # *** 수정이 필요하면 맞게 수정하세요 ***\n",
    "\n",
    "    # If starting from imagenet, train heads only for a bit\n",
    "    # since they have random weights\n",
    "    \n",
    "    print(\"Train network heads\")\n",
    "    model.train(dataset_train, dataset_val,\n",
    "                learning_rate=config.LEARNING_RATE, #config 참조 필요 혹은 직접 러닝레이트 지정\n",
    "                epochs=20,\n",
    "                augmentation=augmentation,\n",
    "                layers='heads')\n",
    "\n",
    "    \n",
    "    print(\"Train all layers\")\n",
    "    model.train(dataset_train, dataset_val,\n",
    "                learning_rate=config.LEARNING_RATE,\n",
    "                epochs=100,\n",
    "                augmentation=augmentation,\n",
    "                layers='all')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cKKSIa4Ju4Ef"
   },
   "outputs": [],
   "source": [
    "# In[20]:\n",
    "\n",
    "\n",
    "\n",
    "############################################################\n",
    "#  RLE Encoding\n",
    "############################################################\n",
    "#RLE란 무엇인가? 간단한 비손실 압축 방법으로 데이터에서 같은 값이 연속해서 나타나는 것을 그 개수와 반복되는 값만으로 표시하는 방법.\n",
    "def rle_encode(mask): #mask를 RLE 인코딩 하나보다\n",
    "    \"\"\"Encodes a mask in Run Length Encoding (RLE).\n",
    "    Returns a string of space-separated values.\n",
    "    \"\"\"\n",
    "    assert mask.ndim == 2, \"mask는 다음과 같은 모양이여야 한다. [Height, Width]\"\n",
    "    # Flatten it column wise \n",
    "    m = mask.T.flatten()\n",
    "    # Compute gradient. Equals 1 or -1 at transition points\n",
    "    g = np.diff(np.concatenate([[0], m, [0]]), n=1)\n",
    "    # 1-based indicies of transition points (where gradient != 0)\n",
    "    rle = np.where(g != 0)[0].reshape([-1, 2]) + 1\n",
    "    # Convert second index in each pair to lenth\n",
    "    rle[:, 1] = rle[:, 1] - rle[:, 0]\n",
    "    return \" \".join(map(str, rle.flatten()))\n",
    "\n",
    "\n",
    "def rle_decode(rle, shape): #압축한 mask 인코딩 데이터를 디코딩한다.\n",
    "    \"\"\"Decodes an RLE encoded list of space separated\n",
    "    numbers and returns a binary mask.\"\"\"\n",
    "    rle = list(map(int, rle.split()))\n",
    "    rle = np.array(rle, dtype=np.int32).reshape([-1, 2])\n",
    "    rle[:, 1] += rle[:, 0]\n",
    "    rle -= 1\n",
    "    mask = np.zeros([shape[0] * shape[1]], np.bool)\n",
    "    for s, e in rle:\n",
    "        assert 0 <= s < mask.shape[0]\n",
    "        assert 1 <= e <= mask.shape[0], \"shape: {}  s {}  e {}\".format(shape, s, e)\n",
    "        mask[s:e] = 1\n",
    "    # Reshape and transpose\n",
    "    mask = mask.reshape([shape[1], shape[0]]).T\n",
    "    return mask\n",
    "\n",
    "\n",
    "def mask_to_rle(image_id, mask, scores): #\n",
    "    \"Encodes instance masks to submission format.\"\n",
    "    assert mask.ndim == 3, \"Mask must be [H, W, count]\"\n",
    "    # If mask is empty, return line with image ID only\n",
    "    if mask.shape[-1] == 0:\n",
    "        return \"{},\".format(image_id)\n",
    "    # 겹치는 mask 제거\n",
    "    # Multiply each instance mask by its score order\n",
    "    # then take the maximum across the last dimension #점수 제일 높은걸 고른다는 듯.\n",
    "    order = np.argsort(scores)[::-1] + 1  # 1-based descending\n",
    "    mask = np.max(mask * np.reshape(order, [1, 1, -1]), -1)\n",
    "    # Loop over instance masks\n",
    "    lines = []\n",
    "    for o in order:\n",
    "        m = np.where(mask == o, 1, 0)\n",
    "        # Skip if empty\n",
    "        if m.sum() == 0.0:\n",
    "            continue\n",
    "        rle = rle_encode(m)\n",
    "        lines.append(\"{}, {}\".format(image_id, rle))\n",
    "    return \"\\n\".join(lines)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4i5SZ7FHu6H8"
   },
   "outputs": [],
   "source": [
    "\n",
    "# In[21]:\n",
    "\n",
    "\n",
    "\n",
    "############################################################\n",
    "#  Detection\n",
    "############################################################\n",
    "\n",
    "def detect(model, dataset_dir, subset):\n",
    "    import matplotlib.pyplot as plt\n",
    "    \"\"\"Run detection on images in the given directory.\"\"\"\n",
    "    print(\"Running on {}\".format(dataset_dir))\n",
    "\n",
    "    # Create directory\n",
    "    if not os.path.exists(RESULTS_DIR):\n",
    "        os.makedirs(RESULTS_DIR)\n",
    "    submit_dir = \"submit_{:%Y%m%dT%H%M%S}\".format(datetime.datetime.now())\n",
    "    submit_dir = os.path.join(RESULTS_DIR, submit_dir)\n",
    "    os.makedirs(submit_dir)\n",
    "\n",
    "    # Read dataset\n",
    "    dataset = MangaDataset()\n",
    "    dataset.load_Manga(dataset_dir, subset) #불러옴\n",
    "    dataset.prepare()\n",
    "    # Load over images\n",
    "    submission = []\n",
    "    for image_id in dataset.image_ids:\n",
    "        # Load image and run detection\n",
    "        image = dataset.load_image(image_id)\n",
    "        print(type(image))\n",
    "        # Detect objects\n",
    "        r = model.detect([image], verbose=0)[0]\n",
    "        # Encode image to RLE. Returns a string of multiple lines\n",
    "        source_id = dataset.image_info[image_id][\"id\"]\n",
    "        rle = mask_to_rle(source_id, r[\"masks\"], r[\"scores\"])\n",
    "        submission.append(rle)\n",
    "        # Save image with masks\n",
    "        visualize.display_instances(\n",
    "            image, r['rois'], r['masks'], r['class_ids'],\n",
    "            dataset.class_names, r['scores'],\n",
    "            show_bbox=False, show_mask=True,\n",
    "            title=\"Predictions\")\n",
    "        plt.savefig(\"{}/{}.png\".format(submit_dir, dataset.image_info[image_id][\"id\"]))\n",
    "\n",
    "    # Save to csv file\n",
    "    #submission = \"ImageId,EncodedPixels\\n\" + \"\\n\".join(submission)\n",
    "    file_path = os.path.join(submit_dir, \"submit.csv\")\n",
    "    with open(file_path, \"w\") as f:\n",
    "        f.write(submission)\n",
    "    print(\"Saved to \", submit_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "inP3jNwQel9Y"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configurations:\n",
      "BACKBONE                       resnet50\n",
      "BACKBONE_STRIDES               [4, 8, 16, 32, 64]\n",
      "BATCH_SIZE                     1\n",
      "BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]\n",
      "COMPUTE_BACKBONE_SHAPE         None\n",
      "DETECTION_MAX_INSTANCES        100\n",
      "DETECTION_MIN_CONFIDENCE       0.9\n",
      "DETECTION_NMS_THRESHOLD        0.3\n",
      "FPN_CLASSIF_FC_LAYERS_SIZE     1024\n",
      "GPU_COUNT                      1\n",
      "GRADIENT_CLIP_NORM             5.0\n",
      "IMAGES_PER_GPU                 1\n",
      "IMAGE_CHANNEL_COUNT            3\n",
      "IMAGE_MAX_DIM                  1216\n",
      "IMAGE_META_SIZE                16\n",
      "IMAGE_MIN_DIM                  768\n",
      "IMAGE_MIN_SCALE                0\n",
      "IMAGE_RESIZE_MODE              pad64\n",
      "IMAGE_SHAPE                    [2112 1344    3]\n",
      "LEARNING_MOMENTUM              0.9\n",
      "LEARNING_RATE                  0.0001\n",
      "LOSS_WEIGHTS                   {'rpn_class_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'mrcnn_mask_loss': 1.0}\n",
      "MASK_POOL_SIZE                 14\n",
      "MASK_SHAPE                     [28, 28]\n",
      "MAX_GT_INSTANCES               30\n",
      "MEAN_PIXEL                     [123.7 116.8 103.9]\n",
      "MINI_MASK_SHAPE                (112, 112)\n",
      "NAME                           Manga\n",
      "NUM_CLASSES                    4\n",
      "OPTIMIZER                      ADAM\n",
      "POOL_SIZE                      7\n",
      "POST_NMS_ROIS_INFERENCE        1000\n",
      "POST_NMS_ROIS_TRAINING         2000\n",
      "PRE_NMS_LIMIT                  6000\n",
      "ROI_POSITIVE_RATIO             0.33\n",
      "RPN_ANCHOR_RATIOS              [0.5, 1, 2]\n",
      "RPN_ANCHOR_SCALES              (32, 64, 128, 256, 512)\n",
      "RPN_ANCHOR_STRIDE              1\n",
      "RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]\n",
      "RPN_NMS_THRESHOLD              0.9\n",
      "RPN_TRAIN_ANCHORS_PER_IMAGE    256\n",
      "STEPS_PER_EPOCH                8\n",
      "TOP_DOWN_PYRAMID_SIZE          256\n",
      "TRAIN_BN                       False\n",
      "TRAIN_ROIS_PER_IMAGE           200\n",
      "USE_MINI_MASK                  True\n",
      "USE_RPN_ROIS                   True\n",
      "VALIDATION_STEPS               5\n",
      "WEIGHT_DECAY                   0.0001\n",
      "h                              2112\n",
      "w                              1344\n",
      "\n",
      "\n",
      "(?, 2112, 1344, 3)\n",
      "Loading weights  C:\\Users\\sanghoon\\AssIstoon\\mask_rcnn_manga_2000.h5\n",
      "Running on D:/sfx_extract_adam_plus/dataset/\n",
      "<class 'numpy.ndarray'>\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-da12c9f0d53d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    120\u001b[0m         \u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mcommand\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"detect\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 122\u001b[1;33m         \u001b[0mdetect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'new'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    123\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m         print(\"'{}' is not recognized. \"\n",
      "\u001b[1;32m<ipython-input-26-3f7f81c329ff>\u001b[0m in \u001b[0;36mdetect\u001b[1;34m(model, dataset_dir, subset)\u001b[0m\n\u001b[0;32m     40\u001b[0m             \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclass_names\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'scores'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m             \u001b[0mshow_bbox\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshow_mask\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m             title=\"Predictions\")\n\u001b[0m\u001b[0;32m     43\u001b[0m         \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msavefig\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"{}/{}.png\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubmit_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimage_info\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mimage_id\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"id\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AssIstoon\\sfx_extract\\mrcnn\\visualize.py\u001b[0m in \u001b[0;36mdisplay_instances\u001b[1;34m(image, boxes, masks, class_ids, class_names, scores, title, figsize, ax, show_mask, show_bbox, colors, captions, original_shape)\u001b[0m\n\u001b[0;32m    117\u001b[0m     \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'off'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m     \u001b[0max\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_title\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m     \u001b[0mmasked_image\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muint32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m         \u001b[0mcolor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcolors\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "\n",
    "if __name__ == '__main__':\n",
    "    import argparse\n",
    "    import tensorflow as tf\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    # Parse command line arguments\n",
    "    '''\n",
    "    parser = argparse.ArgumentParser(\n",
    "        description='Mask R-CNN for AssIstoon counting and segmentation')\n",
    "    parser.add_argument(\"command\",\n",
    "                        metavar=\"<command>\",\n",
    "                        help=\"'train' or 'detect'\")\n",
    "    parser.add_argument('--dataset', required=False,\n",
    "                        metavar=\"/path/to/dataset/\",\n",
    "                        help='Root directory of the dataset')\n",
    "    parser.add_argument('--weights', required=True,\n",
    "                        metavar=\"/path/to/weights.h5\", #수정 필요? \n",
    "                        help=\"Path to weights .h5 file or 'coco'\")\n",
    "    parser.add_argument('--logs', required=False,\n",
    "                        default=DEFAULT_LOGS_DIR,\n",
    "                        metavar=\"/path/to/logs/\",\n",
    "                        help='Logs and checkpoints directory (default=logs/)')\n",
    "    parser.add_argument('--subset', required=False,\n",
    "                        metavar=\"Dataset sub-directory\",\n",
    "                        help=\"Subset of dataset to run prediction on\")\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    '''\n",
    "\n",
    "    '''dataset_train = MangaDataset()\n",
    "    dataset_train.load_Manga()\n",
    "    dataset_train.prepare()\n",
    "    print(\"Image Count: {}\".format(len(dataset_train.image_ids)))\n",
    "    print(\"Class Count: {}\".format(dataset_train.num_classes))\n",
    "    for i, info in enumerate(dataset_train.class_info):\n",
    "        print(\"{:3}. {:50}\".format(i, info['name']))\n",
    "\n",
    "    image_ids = np.random.choice(dataset_train.image_ids, 4)\n",
    "    print(image_ids)\n",
    "\n",
    "    for image_id in image_ids:\n",
    "\n",
    "        image = dataset_train.load_image(image_id)\n",
    "        mask, class_ids = dataset_train.load_mask(image_id)\n",
    "        visualize.display_top_masks(image, mask, class_ids,dataset_train.class_names, limit=1)\n",
    "'''\n",
    "    command=\"train\"\n",
    "    command='detect'\n",
    "    # Validate arguments\n",
    "    #if args.command == \"train\":\n",
    "        #assert args.dataset, \"Argument --dataset is required for training\"\n",
    "    #elif args.command == \"detect\":\n",
    "    #    assert args.subset, \"Provide --subset to run prediction on\"\n",
    "\n",
    "    #print(\"Weights: \", args.weights)\n",
    "    #print(\"Dataset: \", args.dataset)\n",
    "        #if args.subset:\n",
    "    #print(\"Subset: \", args.subset)\n",
    "    #print(\"Logs: \", args.logs)\n",
    "\n",
    "    # Configurations\n",
    "    #if args.command == \"train\":\n",
    "    if command=='train':\n",
    "        config = MangaConfig()\n",
    "    else:\n",
    "        config = MangaInferenceConfig()\n",
    "    config.display()\n",
    "\n",
    "    # Create model\n",
    "    #if args.command == \"train\":\n",
    "    if command==\"train\":\n",
    "        model = modellib.MaskRCNN(mode=\"training\", config=config,\n",
    "                                  model_dir=\n",
    "                                \"D:/logs\")\n",
    "    else:\n",
    "        model = modellib.MaskRCNN(mode=\"inference\", config=config,\n",
    "                                  model_dir=\n",
    "                                 \"D:/logs/\")\n",
    "    weights=\"coco\"\n",
    "    # Select weights file to load\n",
    "    #if args.weights.lower() == \"coco\":\n",
    "    if weights==\"coco\":\n",
    "        weights_path = COCO_WEIGHTS_PATH\n",
    "        # Download weights file\n",
    "        if not os.path.exists(weights_path):\n",
    "            utils.download_trained_weights(weights_path)\n",
    "    elif weights.lower() == \"last\":\n",
    "        # Find last trained weights\n",
    "        weights_path = model.find_last()\n",
    "    elif weights.lower() == \"imagenet\":\n",
    "        # Start from ImageNet trained weights\n",
    "        weights_path = model.get_imagenet_weights()\n",
    "    else:\n",
    "        weights_path = args.weights\n",
    "\n",
    "    # Load weights\n",
    "    print(\"Loading weights \", weights_path)\n",
    "    if weights.lower() == \"coco\":\n",
    "        # Exclude the last layers because they require a matching\n",
    "        # number of classes\n",
    "        model.load_weights(weights_path, by_name=True\n",
    "            #, exclude=[\n",
    "            #\"mrcnn_class_logits\",\n",
    "            #\"mrcnn_bbox_fc\",\n",
    "            #\"mrcnn_bbox\",\n",
    "            #\"mrcnn_mask\"\n",
    "            #]\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        model.load_weights(weights_path, by_name=True, exclude=[\n",
    "            \"mrcnn_class_logits\",\n",
    "            \"mrcnn_bbox_fc\",\n",
    "            \"mrcnn_bbox\",\n",
    "            \"mrcnn_mask\"\n",
    "            ])\n",
    "\n",
    "    dataset='D:/sfx_extract_adam_plus/dataset/'\n",
    "    if command == \"train\":\n",
    "        train(model, dataset)\n",
    "    elif command == \"detect\":\n",
    "        detect(model, dataset, 'new')\n",
    "    else:\n",
    "        print(\"'{}' is not recognized. \"\n",
    "              \"Use 'train' or 'detect'\".format(args.command))\n",
    "\n",
    "    # Train or evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-yuvFR65vgzc"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from google.colab import files\n",
    "path='/content/dataset/logs/manga20190728T0623/'\n",
    "filelist=[path+name for name in os.listdir(path)]\n",
    "print(filelist)\n",
    "for file in filelist:\n",
    "  files.download(file) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J7YmDk9aTCqO"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "marcnn train test colab ver.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
